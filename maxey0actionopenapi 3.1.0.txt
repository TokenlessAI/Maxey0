openapi: 3.1.0
info:
  title: Maxey0 Initialization API
  version: 1.0.0
servers:
  - url: https://api.openai.com/v1
paths:
  /initialize_maxey0:
    post:
      summary: Initializes Maxey0 by loading CSV data and setting up the virtual vector graph.
      operationId: initializeMaxey0
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                csv_file_path:
                  type: string
                  description: The path to the CSV file.
      responses:
        "200":
          description: Successful initialization
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: string
                    description: Initialization status message
      x-openai-action:
        function_name: initialize_maxey0
        code: |
          import subprocess
          import sys
          import pandas as pd
          import time
          from urllib import request
          import json

          def install(package):
              subprocess.check_call([sys.executable, "-m", "pip", "install", package])

          install('pandas')

          class InternalAIAssistant:
              def __init__(self, graph):
                  self.graph = graph
                  self.log = []

              def process_node(self, node_id):
                  node = self.graph.get(node_id)
                  if not node:
                      self.log.append(f"Node {node_id} not found.")
                      return
                  target_value = node['target']
                  solution = f"Solution for node {node_id} with target {target_value}"
                  self.graph[node_id]['solution'] = solution
                  self.log.append(f"Processed node {node_id}: {solution}")

              def process_all_nodes(self):
                  for node_id in list(self.graph.keys()):
                      if isinstance(node_id, int):
                          self.process_node(node_id)
              
              def get_log(self):
                  return self.log

              def maximum_depth_qrq(self, topic):
                  refined_questions = [
                      f"Layer 1: What is the fundamental aspect of {topic}?",
                      f"Layer 2: How does {topic} impact other variables?",
                      f"Layer 3: What are the limitations of current knowledge on {topic}?",
                      f"Layer 4: How can the understanding of {topic} be improved?",
                      f"Layer 5: What are the future directions for research on {topic}?"
                  ]
                  for question in refined_questions:
                      self.log.append(f"QRQ: {question}")
              
              def perform_qrq(self, query):
                  training_data = pd.read_csv('/mnt/data/generated_smaller_diverse_dataset.csv')
                  search_url = f"http://example.com/search?q={query}"
                  response = request.urlopen(search_url)
                  web_data = json.load(response)
                  
                  combined_data = {
                      "training_data": training_data.to_dict(orient="records"),
                      "web_data": web_data
                  }
                  
                  refined_query = f"Refined results for {query}"
                  self.log.append(f"QRQ on {query}: {refined_query}")
                  return refined_query, combined_data

          def initialize_session(csv_file_path):
              print("Initializing Maxey0... This may take a while.")
              start_time = time.time()
              csv_data = pd.read_csv(csv_file_path)
              vector_graph = {index: row.to_dict() for index, row in csv_data.iterrows()}

              ai_assistant = InternalAIAssistant(vector_graph)
              ai_assistant.process_all_nodes()
              ai_assistant.maximum_depth_qrq("initial topic")

              vector_graph['operation_log'] = ai_assistant.get_log()

              def get_size(obj, seen=None):
                  size = sys.getsizeof(obj)
                  if seen is None:
                      seen = set()
                  obj_id = id(obj)
                  if obj_id in seen:
                      return 0
                  seen.add(obj_id)
                  if isinstance(obj, dict):
                      size += sum([get_size(v, seen) for v in obj.values()])
                      size += sum([get_size(k, seen) for k in obj.keys()])
                  elif hasattr(obj, '__dict__'):
                      size += get_size(obj.__dict__, seen)
                  elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):
                      size += sum([get_size(i, seen) for i in obj])
                  return size

              num_nodes = len(vector_graph) - 1
              size_bytes = get_size(vector_graph)
              size_gb = size_bytes / (1024 ** 3)

              end_time = time.time()
              init_duration = end_time - start_time

              print(f"Size in nodes: {num_nodes}")
              print(f"Size in GB: {size_gb}")
              print(f"Initialization time: {init_duration} seconds")
              print("Maxey0 is ready to operate at lightning speed.")
              
              return vector_graph, init_duration

          csv_file_path = params['csv_file_path']
          vector_graph, init_duration = initialize_session(csv_file_path)

          def respond(prompt, init_duration):
              response_time = init_duration / 4
              time.sleep(response_time)
              return f"Response to: {prompt}"

          prompt = "What is the target for node 0?"
          response = respond(prompt, init_duration)
          print(response)
  /maximum_depth_qrq:
    post:
      summary: Performs Maximum Depth QRQ on training data, the Knowledge CSV, and the web.
      operationId: maximumDepthQRQ
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                query:
                  type: string
                  description: The query to refine and process.
      responses:
        "200":
          description: Successful QRQ
          content:
            application/json:
              schema:
                type: object
                properties:
                  refined_query:
                    type: string
                    description: Refined query result
                  data:
                    type: object
                    description: Combined data from training and web sources
      x-openai-action:
        function_name: maximum_depth_qrq
        code: |
          import pandas as pd
          from urllib import request
          import json

          def fetch_web_data(query):
              # This is a placeholder for actual web fetching logic
              search_url = f"http://example.com/search?q={query}"
              response = request.urlopen(search_url)
              return json.load(response)

          query = params['query']

          ai_assistant = InternalAIAssistant({})
          refined_query, combined_data = ai_assistant.perform_qrq(query)

          return {
              "refined_query": refined_query,
              "data": combined_data
          }